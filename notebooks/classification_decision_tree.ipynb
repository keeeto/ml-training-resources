{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Decision Trees\n",
    "\n",
    "Decision trees are among the most popular classical machine learning algorithms, which can used for both classification (so-called classification trees) and regression (so-called regression trees). In this notebook, we will use decision trees for classification. \n",
    "\n",
    "Our implementation will be based on the open source machine learning library [scikit-learn](https://scikit-learn.org/stable/index.html). For simplicity and interpretability, we will use some synthetic 2D data in this notebook. In the practical session, we will use some scientific data to learn regression with decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import sklearn.datasets\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "\n",
    "We start by generating a dataset using the `make_blobs` method from `sklearn`. This method can generate multi-dimensional random data points (blobs) in a Gaussian distribution around a specific number of centres.\n",
    "\n",
    "In the following cell, we generate a dataset of 600 samples in 2D (2 features) with 4 centres (four classes). We will use the first 300 samples for training and the rest 300 for test. The argument `cluster_std` controls the dispersion of the dataset: a larger `cluster_std` makes the dataset more challenging to classify. Change `cluster_std` to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Gaussian blobs\n",
    "all_X, all_y = make_blobs(n_samples=600, n_features=2, centers=4,\n",
    "                          cluster_std=2, random_state=0)\n",
    "# train set\n",
    "X, y = all_X[:300], all_y[:300]\n",
    "\n",
    "# plot data points\n",
    "plt.figure(dpi=100)\n",
    "scat = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n",
    "                                title='Classes', bbox_to_anchor=(1.2, 1.)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "## 1. Create a decision tree classifier\n",
    "\n",
    "Now we create a `DecisionTreeClassifier` object from `sklearn` to classify the blobs. There are many hyperparameters that can be set for the classifier. We will explain some important ones below. For a full list see [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "* **`max_depth`**\n",
    "\n",
    "The maximum depth of the tree. If left as `None`, nodes are expanded until all leaves are pure or until all leaves contain fewer than `min_samples_split` samples. Here we use `None`, the default value.\n",
    "\n",
    "* **`min_samples_split`**\n",
    "\n",
    "The minimum number of samples required to split an internal node. Here we use `min_samples_split=2`, the default value, so our tree will be a binary tree.\n",
    "    \n",
    "* **`max_features`**\n",
    "\n",
    "The maximum number of features considered for splitting. If left as `None`, the classifier will use `n_features` of the input data. Here we use `None`, the default value, so 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree\n",
    "decision_tree = DecisionTreeClassifier(max_depth=None, min_samples_split=2, max_features=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit the model to data\n",
    "\n",
    "In this instance, we do not need to do much feature engineering because our data are relative simple with only two features and four classes. The `DecisionTreeClassifier` class has a `fit()` method, which takes the feature `X` and label `y` as inputs and fits the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit decision tree\n",
    "tree = decision_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function can be used to visualise the result of the classification. It plots the training data points over a contour plot of model predictions over the entire space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualise classification\n",
    "def visualize_classifier(X, y, model, ax=None, cmap='viridis'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # plot the training points\n",
    "    scat = ax.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.7, edgecolors='k',\n",
    "                      clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create a structured grid of the entire space\n",
    "    x0, x1 = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n",
    "    \n",
    "    # compute model predictions on the grid\n",
    "    y_pred = model.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\n",
    "\n",
    "    # create a contour plot of the grid\n",
    "    n_classes = len(np.unique(y))\n",
    "    ax.contourf(x0, x1, y_pred, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap, zorder=1)\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise classification\n",
    "plt.figure(dpi=100)\n",
    "visualize_classifier(X, y, tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the classification looks rather arbitrary in the central region. To verify the robustness of the model, let us try re-running `fit()` with some random sub-sets of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# number of subsets\n",
    "n_subsets = 6\n",
    "\n",
    "# number of points in a subset\n",
    "n_points_subset = 200\n",
    "\n",
    "# fit with subsets and plot\n",
    "fig, axs = plt.subplots(n_subsets // 3, 3, figsize=(13, n_subsets), dpi=100)\n",
    "for ax in axs.flatten():\n",
    "    # randomly choose data\n",
    "    sub_indices = np.random.choice(np.arange(len(y)), n_points_subset)\n",
    "    # fit and plot\n",
    "    tree = decision_tree.fit(X[sub_indices], y[sub_indices])\n",
    "    visualize_classifier(X[sub_indices], y[sub_indices], tree, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our classification is highly inconsistent in the central region where the different classes mix. It is concerning that the details of the classification varies too drastically with the training data. This is a classic sign that the model is **overfitting**.\n",
    "\n",
    "In this example, we can see visually that overfitting is occurring, but is there a systematic way of detecting overfitting? For this purpose, we first need a **metric** as an objective measurement of how well the model is performing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model evaluation\n",
    "    \n",
    "We can use the `accuracy_score` metric from `sklearn` to quantify the accuracy of the model on this data. Before computing `accuracy_score`, we first re-fit the model with the original `(X, y)` and make the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit decision tree\n",
    "tree = decision_tree.fit(X, y)\n",
    "\n",
    "# make predictions\n",
    "y_pred = tree.predict(X)\n",
    "\n",
    "# compute accuarcy score\n",
    "print('Accuracy score on training data: %.2f' % accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty accurate! Wait up a moment -- let us try the model on the test set (the rest 300 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_X, test_y = all_X[300:], all_y[300:]\n",
    "\n",
    "# plot test data\n",
    "plt.figure(dpi=100)\n",
    "scat = plt.scatter(test_X[:, 0], test_X[:, 1], c=test_y, s=20, alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n",
    "                                title='Classes', bbox_to_anchor=(1.2, 1.)))\n",
    "plt.show()\n",
    "\n",
    "# make predictions with test data\n",
    "test_y_pred = tree.predict(test_X)\n",
    "\n",
    "# compute accuarcy score with test data\n",
    "print('Accuracy score on test data: %.2f' % accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not quite as good as before! The model is overfitting: it is performing well on the training set but not generalising beyond it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter tuning and cross-validation\n",
    "\n",
    "Overfitting often results when the model has too many parameters and thus fits too exactly to the training set. To alleviate overfitting, we must choose a proper number of parameters in the model. For the `DecisionTreeClassifier` class, it is controlled by the `max_depth` argument. We are going to use a training/test split and cross-validation to tune this hyperparameter. The same method can be applied to the other hyperparameters. \n",
    "\n",
    "Below we set up a search across `max_depth` ranging from 1 to 20 by using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) method. For each value, we use a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid to search\n",
    "# here we only search max_depth\n",
    "param_grid = {'max_depth': range(1, 20)}\n",
    "\n",
    "# decision tree object\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# assoicte the gird with the object\n",
    "search = GridSearchCV(decision_tree, param_grid, cv=5)\n",
    "\n",
    "# perform search\n",
    "gs = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the search result\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(search.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.xticks(range(0, 19, 2), range(1, 20, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cross-validation score reaches its maximum at `max_depth=3`. We take the best performing hyperparameter and retrain the model on the full dataset. Then we can apply this tuned model to the test data and see if the accuracy improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best performing hyperparameter\n",
    "best_max_depth = gs.best_params_['max_depth']\n",
    "print('Best max_depth: %d' % best_max_depth)\n",
    "\n",
    "# create and fit model with the best performing hyperparameter\n",
    "tree = DecisionTreeClassifier(max_depth=best_max_depth).fit(X, y)\n",
    "\n",
    "# make predictions with test data\n",
    "test_y_pred = tree.predict(test_X)\n",
    "\n",
    "# compute accuarcy score with test data\n",
    "print('Accuracy score on test data: %.2f' % accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bagging and boosting\n",
    "\n",
    "We can improve the performance of decision trees in two ways, *bagging* and *boosting*, both belonging to ensemble learning algorithms. \"**Ensemble**\" means that the algorithm will plant not just one tree but an ensemble of trees. \n",
    "\n",
    "**Bagging** (or bootstrap aggregation) reduces the variance of a decision tree by training an ensemble of trees on random subsets of the training data (with some replacement) and taking the average of the ensemble as the final result. A **Random Forest** is an example of bagging: in addition to a random subset of data, a random forest uses a random subset of features to grow each tree in the ensemble. \n",
    "\n",
    "**Boosting**, as another ensemble technique, grows the trees in a sequential manner, that is, the misclassified samples or errors in the prior tree will be fed into the current tree and so forth; at the end, the final result is determined by majority voting from the ensemble. **AdaBoost** and **Gradient Boosting** are the most popular boosting algorithms. From a data perspective, the data points are weighted equally in bagging and unequally in boosting, as the former is parallel and the latter sequential. \n",
    "\n",
    "Here we will briefly demonstrate the usage of `RandomForestClassifier`  and `AdaBoostClassifier` from `sklearn`. Gradient boosting will be used for regression in the practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit a random forest classifier\n",
    "forest = RandomForestClassifier(n_estimators=300, random_state=0).fit(X, y)\n",
    "\n",
    "# visulise model\n",
    "plt.figure(dpi=100)\n",
    "visualize_classifier(X, y, forest)\n",
    "plt.show()\n",
    "\n",
    "# make predictions with test data\n",
    "test_y_pred = forest.predict(test_X)\n",
    "\n",
    "# compute accuarcy score with test data\n",
    "print('Accuracy score on test data: %.2f' % accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit a AdaBoost classifier\n",
    "ada = AdaBoostClassifier(n_estimators=300, random_state=0).fit(X, y)\n",
    "\n",
    "# visulise model\n",
    "plt.figure(dpi=100)\n",
    "visualize_classifier(X, y, ada)\n",
    "plt.show()\n",
    "\n",
    "# make predictions with test data\n",
    "test_y_pred = ada.predict(test_X)\n",
    "\n",
    "# compute accuarcy score with test data\n",
    "print('Accuracy score on test data: %.2f' % accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises \n",
    "\n",
    "There are many standard \"toy\" datasets for practicing classification with machine learning, such as `iris`, `wine` and `breast_cancer`. Many of them can be directly loaded from `sklearn`. Try to solve one or some of these problems using decision trees (either single or ensemble). Among these datasets, `iris` is the simplest one with the fewest features and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
